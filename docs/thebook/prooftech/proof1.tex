\chapter{Basic proof techniques.}\label{chap:Basicproof}

\epigraph{Contrariwise,' continued Tweedledee, 'if it was so, it might be; and if it were so, it would be; but as it isn't, it ain't. That's logic.}{Lewis Carroll}

\section{Motivational Speeches}

 Lewis Caroll,  Oxford Logician and lecturer, delivers a self-deprecating jibe  through the words of Tweedledee. He understood very well the following  simple fact: Logic is hard and often sounds like complete gobbledygook. Nevertheless, Mathematical Logic plays the role of grammar for Mathematics and I hope that by the end of the chapter the reader will disagree with Tweedledee. This also the first place 
 
 

The English language tends to be more nuanced than mathematical logic. For example consider the following   internet meme\footnote{My son Luca showed it to me.}:

The phrase
``I never said she stole my money.'' has 7 different meanings depending on the emphasis. For example ``{\bf I } never said she stole my money'' means perhaps somebody else said it, ``I never said {\bf she} stole my money'' might mean I said that somebody else stole the money  while ``I never said she stole my {\bf money}.'' might means  she stole something else.

 Mathematical logic is a lot more precise than vernacular language. Every statement has to be either true or false. Nuances have no place in logic. You have to formulate statements offering only one interpretation. You will be introduced to the main concepts in the following	chapters. You will also learn about Spatchcoq at the same time.

\section{Propositional Calculus}
\epigraph{Nothing goes over my head. My reflexes are too fast, I would catch it and I would kill it. }{ Drax the Destroyer.}

Propositions form the the building bricks of Mathematical Logic. Not any statement is a proposition, like bricks, propositions need to be build certain specifications in order to hold the edifice of Mathematics. The specifications are quite direct:
\begin{Definition}
A proposition is a sentence which is either true or false but not both.
\end{Definition}

 In other words Mathematical logic is completely literal. No metaphors or second meanings there. Drax the Destroyer likes propositional logic. 
This seems like an rather pompous definition and it seems rather limiting (and prone to endless jokes in the case of poor Drax) but it is very important for what follows. Here are some examples of propositions:
\begin{itemize}
\item Earth is a planet.
\item $2+2=5$
\item $\forall x \in \mathbb{R}, x^2 \ge 0$
\item Men are mortal.
\end{itemize}
The common language is much richer than mere logic, There are questions, metaphors, hyperbolae, oratorical flourishes and so on. Here are some  examples of sentences that fail to qualify as propositions. 

\begin{itemize}

\item What time is it?
\item  We are better off today than 3 years ago.
\item  All the world?s a stage, and all the men and women merely players.
\item $x+3>5$
\item It will rain tomorrow.
\item In the case of Drax there is also the issue of ``Metaphors are gonna go over his head.''
\end{itemize}

But why should one restrict to such mundane requirements? Is it not true that if your restrict your language to the direct and practical, avoid questions, stylistics and oratorial style and state only facts then the language is made of propositions only? Enter an old friend of the sophists, the paradox.
\paragraph{\bf Of paradoxes }
In Titus 1:12-13, the apostle Paul states:
``One of themselves, even a prophet of their own, said, The Cretians are alway liars [...] This witness is true''.  The Cretan Philosopher  Epimenides, the prophet in Paul's text, is usually credited with the first version of the paradox.   For simplicity we look at a slightly modified version:  ``This statement is a lie''. Cicero \cite{Cic} tells us how to think about it ``If you say that you lie and you speak the truth, you lie. But then you say that you lie and you speak the truth, so you lie.''

A modern  take  on the old story is Pinnochio's paradox. Assume that Pinnochio utters the statement: ``My nose grows!'', what happens? If we observe his nose growing then his words ring true and so his nose should have remained proportional to his face. If we see no change in the size of the nose then his statement must have been false and so, as the story tells, his nose is due for a growth spur ...  

Another variant is attributed to Bertrand Russell:
'' Once upon a time there was a small island with extreme trespassing laws. Everyone caught trespassing was interrogated.  If   found to be truthful the guards  will shoot the intruder and if the intruder is found lying the  hangman awaits  them. A logician caught trespassing happily shouts ``You will hang me!''. Of course, in the logic of this island a trespasser  is either shot or hanged. If they hang him then the statement is truthful and so they should have  shot him. If they shoot him then his statement is not truthful so they should have hanged him. The law appears  inconsistent.
These  self referential paradoxes  lead to Russell type paradoxes (see Subsection~\ref{subs:Constructive vs Classical} and 
 Chapter~\ref{ch:settheory} as well as Appendix~\ref{chap:setsvstypes})


  \subsection{Connectors}
Just like for other mathematical concepts, we build a ``calculus'' for dealing with  propositions. Some form of this date back to Aristotle but logicians  modified them thorough the ages. As with any kind of language, we start with simple forms and combine them into more complicated ones. In fact, as usual in the  pedagogical process,  we start with complicated propositions and break them down into  standard combinations of simple forms. To do so define five connectors (operators) on the set of propositions: and ($\land$), or ($\lor$),  implication ($\rightarrow$),   negation( $\neg$) and  double implication ($\leftrightarrow$) . The first four are  ``binary operators'', they combine two propositions . The last one is a ``unary operation''  much like minus is for numbers. Most of these notions will seem familiar to you, our approach will however be a little bit more formal. The symbols are not independent, one can express some of them using the others.

\paragraph{\bf Conjunction (and)}
If $P$ and $Q$ are two propositions then their conjunction, denoted by $P\land Q$  (read ``P {\bf and} Q'') is a proposition that is only true if both $P$ and $Q$ are true. We can write the Truth table bellow:

$$\begin{array}{|c|c|c|}\hline P & Q & P\land Q \\\hline T & T & T \\\hline T & F & F \\\hline F & T & F \\\hline F & F & F\\ \hline\end{array}$$
here are some examples.

\begin{itemize}
\item ``She is both intelligent and hard working'' can be written as (``She is intelligent'')$\land$(``She is hard working'').
\item $0<4<5$ can be written as $(0<4)\land (4<5)$.
\end{itemize}

\paragraph{\bf Disjunction (or)}
 if $P$ and $Q$ are two propositions then their disjunction, denoted by $P\lor Q$ (read `` P {\bf or} Q'' )is a proposition that is only false if both $P$ and $Q$ are false. That is it is true if either $P$ or $Q$ or both are true.  Note that in SpatchCoq you can enter this by either clicking on the symbol or by typing \textbackslash /.
 
 The corresponding truth table is: 
 $$\begin{array}{|c|c|c|}\hline P & Q & P\lor Q \\\hline T & T & T \\\hline T & F & T \\\hline F & T & T \\\hline F & F & F\\ \hline\end{array}$$
 
 Here are some examples:

\begin{itemize}
\item ``He is either at work or on his way home'' can be written as (``He is  at work'')$\lor$(``He is on his way home'').
\item $0\le 4$ can be  written as $(0<4)\lor (0=4)$ 
\end{itemize}
Note that, unlike in nature language, the connector $\lor$ is not an ``exclusive or''. The proposition $P\lor Q$ is true in the case both $P$ and $Q$ are true.


\paragraph{\bf Implication} If $P$ and $Q$ are two propositions then  $P\rightarrow Q$  (read ``P {\bf implies} Q ''or ``{\bf if} P {\bf then} Q'') is a proposition that is only false if  $P$ is true and $Q$ is false. In other words, we can produce the following truth table':
$$\begin{array}{|c|c|c|}\hline P & Q & P\rightarrow Q \\\hline T & T & T \\\hline T & F & F \\\hline F & T & T \\\hline F & F & T\\ \hline\end{array}$$

\begin{itemize}
\item ``If it rains then you need your umbrella'' can be written as (``It rains'')$\rightarrow$(``you need your umbrella'').

\end{itemize}
This is the most counterintuitive of all connectors. Note that if the proposition $P$ is false then $P\rightarrow Q$ is automatically true regardless of the truth value of $Q$.

\paragraph{\bf Negation}
if $P$ is a proposition then its negation, denoted by $\neg P$  ( read ``{\bf not} P'') is a proposition that is  false if  $P$ is true and true if $P$ is false. Note that in SpatchCoq this can be typed by clicking on the symbol or by writing not.
The truth table is very simple:
$$\begin{array}{|c|c|}\hline P & \neg P\\\hline T & F \\\hline F & T\\ \hline\end{array}$$



\begin{itemize}
\item ``It is not raining'' can be written as $\neg$(`` It rains'').
\item $0\le 4$ can be  written as $\neg (0>4)$ 



\paragraph{\bf Double implication (If and only if)}

If $P$ and $Q$ are propositions then $P\leftrightarrow Q$ (read ``P {\bf  if and only if} Q'') is the same construction as $(P\rightarrow Q) \land (Q\rightarrow P)$. It means that the two propositions have the exact same truth value. The truth table is:

$$\begin{array}{|c|c|c|}\hline P & Q & P\leftrightarrow Q \\\hline T & T & T \\\hline T & F & F \\\hline F & T & F \\\hline F & F & T\\ \hline\end{array}$$


As you saw in the definition of $\leftrightarrow$, the various connectors are not independent. For example note the following truth tables:

$$\begin{array}{|c|c|c|c|c|}\hline P & Q & P\rightarrow Q & \neg P & (\neg P)\lor Q \\ \hline T & T & T &F &T\\\hline T & F & F &F &F \\\hline F & T & T &T &T \\\hline F & F & T &T&T\\ \hline\end{array}$$


$$\begin{array}{|c|c|c|}\hline P & \neg P & \neg (\neg P) \\\hline T & F & T \\\hline F & T & F \\\hline\end{array}$$

$$\begin{array}{|c|c|c|c|c|c|c|}\hline P & Q & \neg P & \neg Q & P\lor Q & \neg ( P \lor Q) & (\neg P)\land \neg Q \\ \hline T & T & F &F & T & F & F \\\hline T & F & F &T & T &F & F \\\hline F & T & T & F & T & F & F \\\hline F & F & T &T&F& T & T\\ \hline\end{array}$$


Two composed propositions that have the exact truth values regardless of the values of their components are called equivalent. In other words the statement $P \rightarrow Q$ is equivalent to $\neg P \lor Q$, the propositiont $P$ is equivalent to  $\neg \neg P$ and  $ \neg ( P \lor Q)$ is equivalent to proposition $(\neg P)\land \neg Q $.

A proposition that is equivalent to the proposition True (i.e one that is True regardless of the value of its components) is called a tautology. For example, if $P$ and $Q$ are equivalent then $(\neg P) \lor Q$ is a tautology. In particular  $$ ( P \lor Q) \lor (\neg P\land \neg Q)$$ is a tautology. 

\subsection{Translation between english and propositional calculus}

Many questions in real life are not immediately expressed as an easy propositional calculus term and, depending on how convoluted the text is, it  might be a challenge to translate it into one.  Here are some rules that will help you recognise the logical connectors.


\paragraph{\bf The conjunction ($q\mathbf{\land}p$)} This usually appears as ``and'' in texts. In other words  ``It rains and it's windy can pe written as ``It rains'' $ \land $ ``it's windy'. Other forms: ``but'',``moreover," ``however'',  ``even though'', ``although'', ``nevertheless''. Some of these seem surprising because the suggest a certain implication. For example the statement ``It is sunny although cold.'' should be interpreted as ``it is sunny'' $\land$
 ``it is cold''. Even more bizarrely, you can rewrite the above as ``It is sunny even if it's cold.'' using the word ``if'' which suggests an implication.
 
 \paragraph{\bf The disjunction \bf$p \mathbf{\lor}q$}You usually find this as ``or'' but this is a tricky one. In Mathematical logic, $\lor$ is an ``inclusive disjunction''. This means that if both $p$ and $q$ are true then so is ``p $\lor$ q''. In colloquial English ``or'' might mean exclusive disjunction, that is the statement is true only if exactly one of the components holds true. Often this appears in the ``either ... or ...'' syntagm. Moreover ``unless'' sometimes apears as an $\lor$ replacement and some other times as an exclusive disjunction. The translation of or is much more context dependent.  Drax the Destroyer really dislikes this.
 
 \paragraph{\bf The implication $p \rightarrow q$} is the most complicated connector, its' normal form is  "if ... then ..."  But it can also appear as , "p implies q", "p therefore q", "p hence q", "q if p", "q provided p",  "p is the sufficient for q", and "q is the necessary for  p" or "q follows from p". Perhaps the most difficult to understand for is ``p only if q'' as it seems to be saying $q\rightarrow q$ when in fact it says $p\rightarrow q$.
 
\paragraph{\bf The negation $\neg p$} is a bit more standard, the expression is usually $not p$. Not however the more complex forms of ``neither  p nor q '' meaning ``not p $\land$ not q'' or ``not both b and q are true'' which means ``not (p $\land$ q)'' while ``p and q are both not true '' which means ``not p $\land$ not q''.


\paragraph{\bf The double implication $p \leftrightarrow q$} This usually appears as ``p if and only if q'' or ``p is equivalent to q'' or ``p is necessary and sufficient for q''.

 Let us try to apply our newfound understanding. Consider the statement ``if it;'s Tuesday then I have Maths but not English.'' We have 3 propositions here p:``It is Tuesday.'', q:``I have Maths.'' and r:``I have English. ''. The statement can be written as $p\rightarrow (q \land \neg\ r)$.
 
 
 
 
 
 
\subsection{Inference rules}\label{subset:inference}


Of course any argument in propositional logic can be solved with a truth table. However this is quite tedious and hard to extend to more general notions. We prefer to use methods of proof called "inference rules".
Each connector has two rules, an introduction and an elimination rule. We will also describe them using the standard logic notation. More precisely, the notation
$$\infer[name]{Q}{P}$$
means that the inference rule ``name'' allows you to infer Q from P.



 
\rmk{
In brief, the introduction rule of a connector tells you what to do in order to prove a propositions involving the connector.}
 
\rmk{
The elimination rule of a connector tells you how to use a hypothesis involving the connector to prove other things.
 }

 We will list these in the next section. Note that those connectors will be used later in Predicate calculus and there we will be able to give many more examples. We will also take this opportunity to introduce some of SpatchCoq's tactics.

\paragraph{\bf Forward proofs, backward proofs and implication rules}

Let us describe the {\bf implication introduction rule}.  In order to prove the statement $P\rightarrow Q$ we  assume $P$ and try to prove $Q$. In usual logic notation we have:

$$\infer{P\rightarrow Q}{%
    \infer*{Q}{P}
}$$

The equivalent SpatchCoq tactic is ``Assume P then prove Q.''


The {\bf implication elimination rule} is sometimes called ``modus ponens''. If you have the hypothesis $H:P\rightarrow Q$ and the hypothesis $H1:P$ then you can show $Q$. In logical notation

$$\infer{Q}{P\rightarrow Q & P}$$

In SpatchCoq we use the tactic ``Apply result (H H1)'' or ``Apply result H1.'' followed by ``Apply result H.''



Most of the proofs you have seen written in textbooks are written in a style called ``direct proof''. Suppose you have a set of hypotheses and you want to prove a conclusion. You then start from the hypotheses and prove a series of intermediate results that then get added to you hypotheses until you can prove the conclusion. Most of the time in practice however the way you arrive to a proof is  combining that method with another method called ``backward proof''. 

To fix the details we will prove one example, the famous Aristotelian syllogism:

Socrates is a Man.

All men are mortal.

Therefore Socrates is mortal.

We will be somewhat abusive using 3 propositions {\bf Socrates, Man, Mortal}. We will redo this more carefully in Section~\ref{sec:predicatecalculus}.

We have two Axioms, 
$$\mbox{A1 :  \bf Socrates} \rightarrow \mbox{\bf Man}.$$
$$\mbox{A2: \bf  Man} \rightarrow \mbox{\bf Mortal}.$$



And we need to show that $$\mbox{\bf Socrates} \rightarrow \mbox{\bf Mortal}.$$



To do that we need to use Implication introduction, that is we need to assume {\bf Socrates} and try to prove {\bf Mortal}. 

 We start by giving a ``foward proof'' of this. Since we know  A1 and {\bf Socrates}, implication elimination tells us that we have {\bf Man}. Similarly since we know A2 and {\bf Man}, implication elimination gives you {\bf Mortal}, which is what we needed to prove.

We could have argued backwardly as follows: Since we know A2, by implication elimination, in order to prove {\bf Mortal} it is enough to prove {\bf Man}. Similarly from A1 in order to prove {\bf Man} it is enough to prove {\bf Socrates} which we already have as a hypothesis.

In more complicated proofs one often combines the two methods.

Now start Spatchcoq and continue your exploration of Propositional logic using it.

The corresponding argument in SpatchCoq goes as follows. Sset up the three variables:

\inp{Variables Socrates Man Mortal :Prop.}

Note the format: 

Variables $x_{1} x_{2} \cdots x_{n}$ : Prop.

This means the we name these variables and we declare them to be of  {\bf type} Prop. For more discussions about types see \ref{chap:setsvstypes}.
 
 

And then list the two axioms

\inp{Axiom A1 :  Socrates -> Man.}
\inp{Axiom A2 : Man -> Mortal.}


Note the similarity between axioms and variables. See \ref{chap:setsvstypes} for details.

Note the format 

And finally type
\inp{Lemma soc: Socrates->Mortal.}

This time the effect on Spatchcoq si a bit different. For the first time you get to prove something. More precisely the goals window becomes

\coq{ }{Socrates \rightarrow Mortal}

Click on  that statement to get a hint. Pick the first Hint:

\inp{Assume Socrates then prove Mortal.}

This ``tactic'' modifies the goal:

\coq{Hyp:Socrates}{Mortal}

We can now go two ways. 

The first one is a ``forward proof'', very much like the text above, use:

\inp{Obtain Man applying A1 to Hyp.}
to get
\coq{Hyp:Socrates\\ H:Man}{Mortal}
and then 
\inp{Obtain Mortal applying A2 to H.}
and \inp{This follows from assumptions.}
to finish the proof.

The second method is a ``backward proof'', this is a method preferred by Coq and therefore by SpatchCoq.
\begin{proof}\label{backward Socrates}
 
 \inp{Apply result A2.}
 
 to get
\coq{Hyp:Socrates}{Man}

This is equivalent to the above. What we mean is that using A2, we now only need to show Man.

Now we do
 \inp{Apply result A1.}
 
 to get
 
 \coq{Hyp:Socrates}{Socrates}
 which follows by assumption, that is
 \inp{This follows from assumptions.}
 
 \end{proof}
 Of course this is such a simple example that one can do directly 
 \inp{Apply result (A2 (A1 Hyp)).}
 
Congratulations! You have finished your first proof with SpatchCoq. 
 
 
\begin{tcolorbox}[colback=red!5!white,colframe=black]
This might be the place to notice that implication elimination behaves much like a function application in standard mathematics. If you know $H:P\rightarrow Q$  and you know $H1:P$ then $(H H1)$ is a proof for Q. 

Moreover, the labels of the hypotheses are not mere labels. They are objects of the same type as the respective hypothesis. They can be viewed as witnesses for the truth of the respective propositions. Moreover, if we finish our proof with Qed then the name of Lemma itself becomes a witness for its proof.

 
 \end{tcolorbox}
Indeed try
\inp{Lemma soc: Socrates->Mortal.\\
Assume Socrates then prove Mortal.\\
Apply result (A2 (A1 Hyp)).\\
Qed.\\
Print soc.}

to get 

\texttt{
soc =$ \lambda$ Hyp : Socrates, A2 (A1 Hyp) \\
     : Socrates $\rightarrow$ Mortal}

This tells you  that soc is a function  that takes the witness Hyp of the truth of Socrates and produces a witness A2 (A1 Hyp) of the truth of Mortal.
 We will return to types later.





\paragraph{\bf Inference rules for conjunction}
The conjunction introduction says that in order to prove $P\land Q$, you need to prove both $P$ and $Q$. In logic notation we have
$$\infer{P\land Q}{P & Q}$$
In SpatchCoq the tactic we use is ``Prove the conjunction in the goal by first proving P then Q.''

The Conjunction elimination consists of  two separate rules,
$$\infer{P}{P\land Q} \mbox{and} \infer{Q}{P\land Q}$$ To be more precise, if you know $H:P\land Q$ then you can deduce $H1:P$ and $H2:Q$. The corresponding SpatchCoq tactic is ``Eliminate the conjuction in hypothesis H.''

To exemplify this, we shall prove the commutativity of conjunction.
If $P,Q$ are propositions, then $P\land Q \rightarrow Q \land P$. To do so, we use, as above the implication introduction, so we assume that $P\land Q$ holds and show that $Q\land P$. 

Now we will employ to imply the conjunction elimination. Since we know that $P\land Q$ holds, we also know that $P$ holds and that $Q$ holds. by Conjunction introduction we have that $Q\land P$ holds.

The formal proof in SpatchCoq is a bit more elaborate. We start with the Lemma:
\inp{Lemma ancomm(P Q:Prop) : $P\land Q -> Q \land P$.} 

to get \coq{P Q:Prop}{P\land Q \rightarrow Q \land P}

We then use
\inp{Assume ($P \land Q$) then prove ($Q \land P$).}

to get
\coq{P Q:Prop \\ Hyp : P \land Q}{Q \land P}

We know use
\inp{Eliminate the conjuction in hypothesis Hyp.}
To get
\coq{P Q:Prop \\ Hyp0 : P\\ Hyp1:Q}{Q \land P}

Now we use 

\inp{Prove the conjunction in the goal by first proving Q then P.}

To get two goals
\coq{P Q:Prop \\ Hyp0 : P\\ Hyp1:Q}{Q }

and

\coq{P Q:Prop \\ Hyp0 : P\\ Hyp1:Q}{P}

which can each be solved by

\inp{This follows from assumptions.}





\paragraph{Inference rules for disjunction}

The disjunction introduction consists of two different rules. In order to prove $P\/Q$ you can either prove the left hand side or the right hand side.  tHe logical expressions are
$$\infer[left]{P\lor Q}{P} \mbox{ and } \infer[right]{P\lor Q}{Q}.$$

In SpatchCoq we have thee tactics: ``Prove left hand side.'', ``Prove right hand side.'' and 
``Prove * in the disjunction.''

Disjunction elimination is a bit harder to describe but it is a very natural method of ``case by case'' analysis. If you know $H: P\lor Q$ and you  want to prove $R$ then you need to prove $R$ in case $P$ holds as well as in case $Q$ holds.
$$\infer{R}{P\lor Q &\infer{R}{P} & \infer{R}{Q}}$$

In SpatchCoq the tactic is: ``Consider cases based on disjunction in hypothesis H.''

We now give a detailed proof of the commutativity of disjunction:
$$P\lor Q \rightarrow Q\lor P.$$ 

Of course we first assume $P\lor Q$ happens and show $Q \lor P$. To do so we need to argue by cases using Disjunction elimination.

Case 1: P holds. In this case we will prove the right hand side of the disjunction in the goal.This is an assumption and by disjunction intro we are done.

Case 2: Q holds. In this case we will prove the left hand side of the disjunction in the goal. This is an assumption  and by disjunction intro we are done.

Here is the spatchcoq version

\inp{
Lemma ancomm$(P\ Q:Prop):P\lor Q ->Q\lor P.$\\
Assume $(P \lor Q)$ then prove $(Q \lor P)$.\\
Consider cases based on disjunction in hypothesis Hyp.}

at this point, there are two goals generated.

\coq{P\ Q:Prop\\ Hyp0:P}{P\lor Q}
\coq{P \ Q:Prop\\ Hyp1:Q}{P\lor Q}

These are easily eliminated by

\inp{
Prove right hand side.\\
This follows from assumptions.}
respectively 
\inp{
Prove left hand side.\\
This follows from assumptions.}



\paragraph{Inference rules for negation}

Perhaps this is a little hard to digest at first but the negation of $P$ is the same thing as $P\rightarrow False$. Therefore the inference rules for negation are the same as those for implication. In particular, the negation introduction's logic statement is
$$\infer{\neg P}{\infer{False}{P}}.$$
Therefore th This is an important statement to make and, indeed in SpatchCoq in order to deal with negation you will need to use ``Rewrite goal using the definition of not.'' respectively ``Rewrite hypothesis	H using the definition of not.''. To give an example we shall prove 
$$P \rightarrow \neg \neg P$$
We of course first assume $P$ and then prove $\neg \neg P$. To do this we first note that this is the same thing as $(P->False)->False$ and so we assume $P->False$ and try to show $False$. Since now we know $P->False$ and $P$, we can use implication elimination to get False.




The proof in Spatchcoq is identical:

\inp{Lemma notnot$(P:Prop):P \rightarrow \neg \neg P.$\\
Assume P then prove (not (not P)).\\
Rewrite goal using the definition of not.\\
Assume $(P \rightarrow False)$ then prove False.\\
Apply result (Hyp0 Hyp).}

\end{itemize}

\paragraph{Inference rules for equivalence}

We will not insist here because $P\leftrightarrow Q$ is the same as $P\rightarrow Q \land Q\rightarrow P$ and so the inference rules are derivative. In particular
 in spatchCoq we use  tactic :
 
``Prove both directions of P iff Q.''  

as introduction rule in order to prove $P\leftrightarrow Q$ and the  tactic 


``Eliminate the conjuction in hypothesis Hyp.'' 


to eliminate the hypothesis $Hyp: P\leftrightarrow Q$.



 





\subsection{Constructive vs Classical (proofs by contradiction)}\label{subs:Constructive vs Classical}
\epigraph{I mean, you could claim that anything's real if the only basis for believing in it is that nobody's proved it doesn't exist!} 
{J.K. Rowling}
Classical logic includes a certain axiom that the romans called ``tertium non datur'' or ``the excluded middle''. This Axiom states that of $P$ is a proposition then $P\lor \neg P$ always hold. 

 At the beginning of the 20th century a number of mathematicians started debating the need for such an axiom. They came to be collectively called intuitionists. The trouble with that position was that it took away from the power of this axiom without necessarily offering something in return. The things you are able to prove are much more restrictive. As a consequence classical logic carried the day.

However at the end of the century, as Theoretical Computer Science started to gain strength and depth, excluding the excluded middled carried another promise: computability. Via the Curry-Howard correspondence, a ``constructive proof'' (i.e. one without the rule of excluded middle) is equivalent to the construction of a function. In particular, the familiar ``proof by contradiction'' relies on a variant of the excluded middle, namely the fact that the statements $P$ and $\neg \neg P$ are equivalent. We have seen that $P \leftarrow \neg \neg P$ above but the other implication relies on classical logic.

Some ``constructivists'' argue that a proof of $P$ should be a witness to its truth and not merely to the falsity of its negation (as it is the case with $\neg \neg P$.  The motto from JK Rowling does exactly that. This carries quite a bit of weight in the CS world even if not (yet) so much in the Mathematical world.

One of the main methods of "classical logic" is the so called "proof by contradiction". In brief, if you want to prove $P$ then you assume that $P$ is false and then prove a contradiction. The SpatchCoq tactic 
"Prove by contradiction." will transform the statement

\coq{\cdots}{P}

into 
\coq{\cdots\\ H: \neg P}{False}


As an example of proof by contradiction, consider $P$ a proposition and $\neg \neg P$ its double negation. Are these two statements equivalent? We have proved above one of the implications. The converse, however is a bit stranger and requires a proof by contradiction.

\inp{
Lemma oneway (P:Prop): $\neg \neg P\rightarrow P.$\\
Assume (not (not P)) then prove P.}


at this point we have 

\coq{ P:Prop\\
Hyp:not  (not  P)\\
}{P}

so we will employ 
\inp{
Prove by contradiction.}

to get
\coq{ P:Prop\\
Hyp:not  (not  P)\\
H:not P
}{False}
 The rest is quite standard.
\inp{
Apply result Hyp .\\
This follows from assumptions.}

We are not ready to abandon the path of classicism and will assume excluded middle for now. We would however, try to eliminate needlessly using proofs ``by contradiction''.
This is a good point to look at the axiom "classic". If we apply
\inp{Check classic.}

we get the resulst $$classic
    : \forall  P : Prop, P \lor \neg P$$


Therefore, while not an actual independent tactic, applying

"Apply result classic."
will solve any goal that looks like

$$P \lor \neg P.$$

For example let us prove that 
\inp{Lemma a (P Q:Prop): $(P\rightarrow  Q) \rightarrow (\neg P \lor Q)$.
}

We of course first use implication intros by applyiong
\inp{Assume $(P \rightarrow Q)$ then prove $((\neg P) \lor Q)$.}

and get:

\coq{P Q:Prop\\Hyp: P\rightarrow Q}{\neg P \lor Q}

At which point we are stuck without any obvious new possibility to advance. We note however that if P was true then we could use Hyp to obtain Q and if $\neg P$ is true then we would have the disjunction automatically. Thefore we do

 

\inp{Claim ($P \lor \neg P$).\\
Apply result classic.}

To have
\coq{P Q:Prop\\Hyp: P\rightarrow Q\\ H:P \lor \neg P.}{\neg P \lor Q}



We now do a proof by cases that offers no surprises.



\inp{Consider cases based on disjunction in hypothesis H .
Prove right hand side.\\
Apply result Hyp .\\
This follows from assumptions.\\
Prove left hand side.\\
This follows from assumptions.}


Note that the converse is quite easier only requiring  a proof by contradiction.

\inp{Lemma b $(P Q:Prop): (not P \lor  Q)\rightarrow (P\rightarrow Q).$\\
Assume $((not P) \lor Q)$ then prove$ (P \rightarrow  Q).$\\
Assume P then prove Q.\\
Consider cases based on disjunction in hypothesis Hyp .\\
Prove by contradiction.\\
Apply result Hyp1.\\
This follows from assumptions.\\
This follows from assumptions.}


See \ref{sec:drinker} for a rather surprising example of classical logic.

\subsection{Know thine fallacies}

Consider the usual modus ponens rule. 
\infer{P}{\begin{array}{l}Q\\ P \rightarrow Q \end{array}}

Small variations can make this false.
 Consider for example the following argument from Mounty Python and the Holly Grail.
 
 
 
 \infer{\mbox{ It is Tusday }}{\begin{array}{l}\mbox{If it is Tuesday then I play poker }\\ \mbox{I play poker}\end{array}}
 
 
 
 This is one of the most common "formal" fallacy. It is called "Affirming the consequence". Recall that if $P$ is false then $P\rightarrow Q$ is automatically true if $P$ is false and so the deduction above does not hold. It is, however, remarkably prevalent in public discourse especially in adverx

\subsection{A puzzle}

We will now use a puzzle to give a more serious example	 of propositional calculus, its inference rules and their implementation in SpatchCoq. 

The puzzle, ``the lady or the Tiger'' comes from the book  ``The Lady Or the Tiger?: And Other Logic Puzzles'' by Raymond M. Smullyan. It is slightly adapted for the 21st century.

A  prisoner is offered the choice between two doors. Behind each door he could find either the key to his freedom or a very hungry tiger.

 \begin{itemize}

 \item The clue on the first door reads ``the key to your freedom  is in this room and the tiger in the other''.
 \item The clue on the second door reads ``one of the rooms contains  the key to your freedom and the other room the tiger.''
  \item He knows that one of the two clues is correct and the other is incorrect.
 \end{itemize}

What would you do in his place?

We will formalise the questions as follows: We will denote by P the proposition  ``the first room contains the key to freedom'' and by Q the proposition ``the second room contains the key to freedom''. Of course $\neg P$ means ``the first room contains the tiger'' and  $\neg Q$ means ``the second room contains the tiger''.

The clue on the first door is ``the key to your freedom  is in this room and the tiger in the other'' which can be written as $$D1:P\land \neg Q$$.

The second door clue is ``one of the rooms contains  the key to your freedom and the other room the tiger.'' which can be rewritten as ``{\bf either} the first room has the key and the second the tiger {\bf or} the first room has the tiger and the second the key'' so we can write it as:
 $$D2 : (P\land \neg Q) \lor (\neg P \land Q).$$ 
 
 The fact that exactly one clue is correct and the other is incorrect can be written as ``{\bf either} the first door is correct and the second incorrect {\bf or} the first door is incorrect and the second is correct''. This can be written as $(D1 \land \neg D2 )\lor (\neg D1 \land D2)$ which expands to $$((P\land \neg Q)\land \neg ((P\land \neg Q) \lor (\neg P \land Q)))\lor (\neg(P\land \neg Q) \land ((P\land \neg Q) \lor (\neg P \land Q))).$$ 

This looks horrible. We will however show that the second room has the key, that is Q.


For example, the statement we want to prove is 
$$(D1 \land \neg D2 )\lor (\neg D1 \land D2)\rightarrow Q.$$ 
We can set-up SpatchCoq with

\inp{Variables P Q:Prop.}

to define the two propositions $P$ and $Q$.
Then define 

\inp{Definition D1:= $P \land \neg Q.$}

\inp{Definition  D2:= $( \neg P \land Q)\lor(P \land \neg Q)$.}
\inp{Definition onlyone:= $(D1 \land \neg D2)\lor(\neg D1 \land D2)$.}
\inp{Lemma a: onlyone  $\rightarrow Q.$}

After applying the tactic 
\inp{Assume onlyone then prove Q.} we get

\coq{Hyp: onlyone}{Q}
We now use the tactic that we used for not:
\inp{Rewrite hypothesis Hyp  using the definition of onlyone.}
to get

\coq {Hyp: (D1 \land (not \ D2)) \lor ((not\ D1) \land D2)}{Q}

We now use 
\inp{Consider cases based on disjunction in hypothesis Hyp .}
to get two new goals

\coq {Hyp0:D1 \land (not\ D2)}{Q}

and


\coq {Hyp1:not\ D1 \land  D2}{Q}


\inp{Eliminate the conjuction in hypothesis Hyp0.\\
Rewrite hypothesis Hyp1  using the definition of D2.\\
Rewrite hypothesis Hyp  using the definition of D1.
}

brings us to

\coq{Hyp:(P \land (not\ Q))\\
Hyp1: not\ (((not\ P) \land Q) \lor (P \land (not\ Q)))}{Q}

we will now use the proof by contradiction (see \ref{sec:proofbycontradiction})
\inp{
Prove by contradiction.}

to get

\coq{Hyp:(P \land (not\ Q))\\
Hyp1: not\ (((not\ P) \land Q) \lor (P \land (not\ Q)))\\ H:not\ Q}{False}

We note that Hyp1 is of type (not X) that is (X$\rightarrow$False) and so we can apply it (as in the backward proof mentioned above)

\inp{Apply result Hyp1} 
gives


\coq{Hyp:(P \land (not\ Q))\\
Hyp1: not\ (((not\ P) \land Q) \lor (P \land (not\ Q)))\\
H : not\ Q}{((not\ P) \land Q) \lor (P \land (not\ Q))}

Now we note that Hyp is exactly the right hand side of the disjunction so we can use.

\inp{Prove right hand side.\\
This follows from assumptions.}

to finish up this part of the proof.

we are now left with

\coq {Hyp1:not\ D1 \land  D2}{Q}

and, as above we do
\inp{Eliminate the conjuction in hypothesis Hyp1.\\
Rewrite hypothesis Hyp  using the definition of D1.\\
Rewrite hypothesis Hyp0  using the definition of D2.}

to get:
\coq{Hyp:not\ (P \land (not\ Q))\\
Hyp0:(((not\ P) \land Q) \lor (P \land (not\ Q)))\\
}{Q}

Since Hyp0 is a disjunction we do
\inp{Consider cases based on disjunction in hypothesis Hyp0 .}

To get again a case by case analysis.

\coq{Hyp:not\ (P \land (not\ Q))\\
Hyp1:(not\ P) \land Q\\
}{Q}

and

\coq{Hyp:not\ (P \land (not\ Q))\\
Hyp2:P \land (not\ Q)\\
}{Q}

In the first case we use

\inp{Eliminate the conjuction in hypothesis Hyp1 .\\
This follows from assumptions.}

and in the second we prove by contradiction

\inp{Prove by contradiction.\\
Apply result (Hyp Hyp2).\\
Qed.}
\subsection{On the way to the barbershop }

I'd like to thank Richard Kaye for introducing me to this ``paradox''.  In a paper published in 1894 called ``A logical paradox'' Lewis Carroll presents the following situation:

Two uncles want to go the barbershop. There are three barbers, Allen Brown and Carr. We are told that at least one of them has to be in at all times. Also we know that Allen  ``ever since he had that fever he's been so nervous about going out alone, he always takes Brown with him. '' 

One of the uncles then argues that Carr has to be home. The argument is a proof by contradiction. Suppose that Carr is out.  Then if Allen is out then Brown will have to be in since somebody should be in the shop. On the other hand if Allen is out the Brown will have to be out as well on account of Allen's nervousness. Therefore Carr being out generates two contradictory statements ``if Allen is out then Brown is in'' and ``if Allen is out then Brown is out'' and so Carr must be in.

This is a remarkable statement. It is reasonably easy to see what is wrong in today's terms (we shall write a careful argument in a moment) but  at the end of the 19th century  this was serious stumbling block for logicians. In fact in his 1903 book  ``the Principles of Mathematics'', Bertrand  Russell writes:

{\quote  ``The principle that false propositions imply all propositions solves Lewis Carroll's logical paradox in Mind, N. S. No. 11 (1894). The assertion made in that paradox is that, if p, q, r be propositions, and q implies r, while p implies that q implies not-r, then p must be false, on the supposed ground that q implies r and q implies not-r are incompatible. But in virtue of our definition of negation, if q be false both these implications will hold: the two together, in fact, whatever proposition r may be, are equivalent to not-q. Thus the only inference warranted by Lewis Carroll?s premisses is that if p be true, q must be false, i.e. that p implies not-q; and this is the conclusion, oddly enough, which common sense would have drawn in the particular case which he discusses''.}

Indeed the principle that if $p$ is false then $p\rightarrow q$ is true, as seen in the truth table of the implications  was something that was only formalised by Russell. In fact he states that $p\lor q$ is equivalent to $(p \rightarrow q) \rightarrow q$ (see for example \ref{russel}).

Note that B Russell already suggests the answer to the parable. The two statements only prove that if Carr is out then Allen must be in. Let us prove that ins Spatchcoq.

We will define 3 propositions, Allen, Brown and Carr to mean that the corresponding people are in  and state two axioms, $not Allen->not Brown$ and $Allen \lor Brown \lor Carr$ and we prove that $ not Carr \rightarrow Allen$.

\inp{Variables Allen Brown Carr :Prop.\\
Axiom some: Allen $\lor$ Brown $\lor$ Carr.\\
Axiom AB: not Allen ->not Brown.\\
Lemma A: not Carr -> Allen.}

Now we assume that Carr is out and prove that Allen must be in. We use the axiom some to get $Allen \lor (Brown \lor Carr)$.

\inp{Assume (not Carr) then prove Allen.\\
Claim (Allen $\lor$ Brown $\lor$ Carr).\\
Apply result some.}


We get
\coq{Hyp: not Carr\\
H:Allen \lor Brown \lor Carr}{Allen}
The next step is to consider the two  cases: either Allen is home or one of Brown or Carr must be home. If Allen is home then we are done. 
\inp{Consider cases based on disjunction in hypothesis H .\\
This follows from assumptions.
}


We now have
\coq{Hyp: not Carr\\
Hyp1: Brown \lor Carr}{Allen}


We will prove this by contradiction:
\inp{Prove by contradiction.}

To get
\coq{Hyp: not Carr\\
Hyp1: Brown \lor Carr\\ H:not Allen }{False}


We again consider two cases, either Brown happens or Carr happens. In the first case we use the axiom AB

\inp{Consider cases based on disjunction in hypothesis Hyp1 .\\
Apply result AB.\\
This follows from assumptions.
This follows from assumptions.}
 
We are left with the last case:
 
\coq{Hyp: not Carr\\
Hyp2:  Carr\\ H:not Allen }{False} 

Ans so we can finish by applying  modus ponies

\inp{Apply result (Hyp Hyp2).}
 \paragraph{\bf Exercises}\label{prop:exercises}
 \begin{enumerate}
 
 \item[assume] $P \rightarrow P$.
 \item[left]$ P \rightarrow P\lor Q$.
  \item[distr] $P\land (Q\lor R) \rightarrow P\land Q)\lor (P\land R).$
  \item [contrap]$ (P\rightarrow Q) \rightarrow (\neg Q \rightarrow \neg P)$
  \item[implies] $(P\rightarrow Q)\rightarrow (\neg\,P\,\lor \,Q).$
\item[deMorgan] $\neg\,(P\lor Q)\rightarrow (\neg\,P\,\land \neg\,Q).$

  
 \item[impand] $((P \rightarrow Q) \land (P \rightarrow R)) \leftrightarrow (P \rightarrow (Q\land R))$
\item[impor] $((P \rightarrow Q) \lor (P \rightarrow R)) \leftrightarrow (P \rightarrow (Q\lor R))$
\item[andimp] $(P\rightarrow(Q\rightarrow R)) \leftrightarrow ((P\land Q) \rightarrow R)$.
\item[andorimp] $((P \rightarrow R) \land (Q \rightarrow R)) \leftrightarrow ((P \lor Q) \rightarrow R)$
\item[orandimp] $((P \rightarrow R) \lor (Q \rightarrow R)) \leftrightarrow ((P \land Q) \rightarrow R)$
\item[triplenot] $\neg (\neg (\neg P))  \leftrightarrow \neg P$
\item[twoone] $(P \lor Q) \land \neg  P \rightarrow Q$
\item[twotwo] $\neg Q \land (P \rightarrow Q) \rightarrow  \neg P$
\item[twothree] $ C \land (A \rightarrow B) \land (C \rightarrow ( A \rightarrow \neg B)) \rightarrow \neg A$
\item[twofour] $ (P \lor  Q) \land (\neg P \lor R) \rightarrow Q \lor R$.

\item[Russell] \label{russel} $(P\lor Q)\leftrightarrow ((P\rightarrow Q)\rightarrow Q)$



\end{enumerate}
